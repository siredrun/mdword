# 全流程调度

建表

```
mysql -uroot -p1234 -e "CREATE DATABASE gmall_report CHARACTER SET utf8 COLLATE utf8_general_ci;"
# 可以把ods层所有的表都在gmall_report数据库创建，把ods_sql里面的建表语句全部拿出来即可，这里只以创建用户主题表和地区主题表为例。
DROP TABLE IF EXISTS `ads_user_topic`;
CREATE TABLE `ads_user_topic`  (
  `dt` date NOT NULL,
  `day_users` bigint(255) NULL DEFAULT NULL,
  `day_new_users` bigint(255) NULL DEFAULT NULL,
  `day_new_payment_users` bigint(255) NULL DEFAULT NULL,
  `payment_users` bigint(255) NULL DEFAULT NULL,
  `users` bigint(255) NULL DEFAULT NULL,
  `day_users2users` double(255, 2) NULL DEFAULT NULL,
  `payment_users2users` double(255, 2) NULL DEFAULT NULL,
  `day_new_users2users` double(255, 2) NULL DEFAULT NULL,
  PRIMARY KEY (`dt`) USING BTREE
) COMMENT='用户主题表' ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
DROP TABLE IF EXISTS `ads_area_topic`;
CREATE TABLE `ads_area_topic`  (
  `dt` date NOT NULL,
  `id` int(11) NULL DEFAULT NULL,
  `province_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `area_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `iso_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `region_id` int(11) NULL DEFAULT NULL,
  `region_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `order_day_count` bigint(255) NULL DEFAULT NULL,
  `order_day_amount` double(255, 2) NULL DEFAULT NULL,
  `payment_day_count` bigint(255) NULL DEFAULT NULL,
  `payment_day_amount` double(255, 2) NULL DEFAULT NULL,
  PRIMARY KEY (`dt`, `iso_code`) USING BTREE
)COMMENT='地区主题表'  ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
```

hdfs_to_mysql导出Sqoop脚本的简单说明：关于导出update还是insert的问题。
--update-mode：updateonly只更新不新增、allowinsert允许新增 。
--update-key：允许更新的情况下，指定哪些字段匹配视为同一条数据，进行更新而不增加。多个字段用逗号分隔。
--input-null-string和--input-null-non-string：分别表示，将字符串列和非字符串列的空串和“null”转义。
官网地址：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html
Sqoop will by default import NULL values as string null. Hive is however using string \N to denote NULL values and therefore predicates dealing with NULL(like IS NULL) will not work correctly. You should append parameters --null-string and --null-non-string in case of import job or --input-null-string and --input-null-non-string in case of an export job if you wish to properly preserve NULL values. Because sqoop is using those parameters in generated code, you need to properly escape value \N to \\N:
Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。

```
hdfs_to_mysql all # hdfs_to_mysql导出Sqoop脚本，事先启动hadoop集群
select * from ads_user_topic;
select * from ads_area_topic;
```

## 全调度流程涉及脚本

关系展示

| 层级 | 名称                 | 简介                     | 数据类型类型           | 依赖脚本                                                  |
| ---- | -------------------- | ------------------------ | ---------------------- | --------------------------------------------------------- |
| 采集 | gmall_mysql_to_hdfs  | mysql->HDFS，sqoop脚本   | 业务数据               |                                                           |
| ods  | hdfs_to_ods_log      | hive脚本                 | 用户行为数据           |                                                           |
| ods  | hdfs_to_ods_db       | hive脚本                 | 业务数据               | gmall_mysql_to_hdfs                                       |
| dwd  | ods_to_dwd_start_log | 启动表，hive脚本         | 用户行为数据           | hdfs_to_ods_log                                           |
| dwd  | ods_to_dwd_base_log  | 事件日志基础表，hive脚本 | 用户行为数据           | hdfs_to_ods_log                                           |
| dwd  | dwd_events_log       | 事件日志具体表，hive脚本 | 用户行为数据           | ods_to_dwd_start_log                                      |
| dwd  | ods_to_dwd_db        | hive脚本                 | 业务数据               | hdfs_to_ods_db                                            |
| dws  | dwd_to_dws           | hive脚本                 | 用户行为数据和业务数据 | ods_to_dwd_start_log、dwd_events_log(可略)、ods_to_dwd_db |
| dwt  | dws_to_dwt           | hive脚本                 | 用户行为数据和业务数据 | dwd_to_dws                                                |
| ads  | dwt_to_ads           | hive脚本                 | 用户行为数据和业务数据 | dws_to_dwt                                                |
| 调度 | hdfs_to_mysql        | HDFS->mysql，sqoop脚本   | 用户行为数据和业务数据 | dwt_to_ads                                                |

**注意**：层级分为采集=>ods=>dwd=>dws=>dwt=>ads=>调度六个层级；ods_to_dwd_base_log和dwd_events_log这两个脚本在调度全流程中可省略。把下面九个后缀为job的文件全部放在gmall目录，打包为gmall.zip，因为目前azkaban内置的任务类型支持的command中，Azkaban上传的工作流文件只支持xxx.zip文件。zip应包含xxx.job运行作业所需的文件和任何文件（文件名后缀必须以.job结尾，否则无法识别）。作业名称在项目中必须是唯一的。下面Job的配置请结合上面的关系展示表格理解，脚本的路径请改为自己的路径。最后：关于省略的脚本ods_to_dwd_base_log和dwd_events_log想加的话也可以加在job里。

（1）gmall_mysql_to_hdfs.job

```sh
type=command
/opt/module/myscripts/gmall_mysql_to_hdfs all ${dt}
```

（2）hdfs_to_ods_log.job

```sh
type=command
/opt/module/myscripts/hdfs_to_ods_log ${dt}
```

（3）hdfs_to_ods_db.job，注意dependencies依赖

```sh
type=command
/opt/module/myscripts/hdfs_to_ods_db all ${dt}
dependencies=gmall_mysql_to_hdfs
```

（4）ods_to_dwd_start_log.job

```sh
type=command
/opt/module/myscripts/ods_to_dwd_start_log ${dt}
dependencies=hdfs_to_ods_log
```

（5）ods_to_dwd_db.job

```
type=command
/opt/module/myscripts/ods_to_dwd_db all ${dt}
dependencies=hdfs_to_ods_db
```

（6）dwd_to_dws.job 依赖有多个用逗号分开，dwd_events_log略

```sh
type=command
/opt/module/myscripts/dwd_to_dws ${dt}
dependencies=ods_to_dwd_db,ods_to_dwd_start_log
```

（7）dws_to_dwt.job

```sh
type=command
/opt/module/myscripts/dws_to_dwt ${dt}
dependencies=dwd_to_dws
```

（8）dwt_to_ads.job

```sh
type=command
/opt/module/myscripts/dwt_to_ads ${dt}
dependencies=dws_to_dwt
```

（9）hdfs_to_mysql.job

```sh
type=command
/opt/module/myscripts/hdfs_to_mysql all
dependencies=dwt_to_ads
```

## 调度开始

0.生成对应日期的业务数据和用户行为数据

```
# 用户行为数据 确保hd zk kf启动
lg # hadoop102和hadoop103的/tmp/logs/生成日志
f1 start # 把hadoop102和hadoop103的日志采集过滤并放到kafka主题topic_start、topic_event里，确保flume的lib下有日志过滤flume-interceptor生成的jar包
# 启动两个消费者分别消费kafka上的topic_start、topic_event里的日志数据，存到hdfs里面。
flume-ng agent -f $FL_HOME/conf/kafka-flume-hdfs.conf -n a1 -Dflume.root.logger=INFO,LOGFILE 
# 确保hdfs下/origin_data/gmall/log/topic_start或topic_event下有后缀为lzo的数据
# 业务数据，在hadoop104执行。确保hadoop102的/opt/module/db_log下gmall-mock-db.jar
vim /opt/module/db_log/application.properties# 修改文件里的mock.date为当前日期，和生成日志的日期一致

jar -jar /opt/module/db_log/gmall-mock-db.jar
mysql -uroot -p1234 -e "use gmall;select * from user_info where create_time = 'mock.date日期' limit 1;"
```

1.启动azkaban-executor-start.sh和azkaban-web-start.sh，访问https://hadoop102:8443/（注意https）输入admin/admin登录；

2.azkaban web客户端中Projects栏目下创建项目，点击Create Project，填写Name为gmall；Description为gmall；

3.Projects栏目下点击创建好的项目gmall进去，上传压缩好的gmall.zip到gmall项目。azkaban会自动校验zip包是否有问题；

4.点击 Execute Flow会出现下图，展示脚本之间的依赖关系，schedule定时调度、execute马上执行，生产环境选择schedule定时调度，测试环境选择execute马上执行。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff09c8ad58de4e929948eb695a038aa4~tplv-k3u1fbpfcp-zoom-1.image)

5.job文件里面有${dt}，Flow Parameters里面添加add rows，名称为dt。如果是生产环境执行，dt无需设置值，如果是测试，dt必须设置值，如值为2020-03-11。设置好dt

```sql
use gmall_report;
delete from ads_user_topic where dt = '2020-08-05';
delete from ads_area_topic where dt = '2020-08-05';
```

设置好以后点击execute再点击continue执行，执行完成大概需要20-60分钟。

6.执行完毕，最后hdfs_to_mysql的job变为绿色表示全部流程执行成功。

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0652ddace9b6401cb21cc1a0b26b8f9b~tplv-k3u1fbpfcp-zoom-1.image)

执行列表

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b46e506683e44e5c89810767d08e4188~tplv-k3u1fbpfcp-zoom-1.image)

查询

```sql
use gmall_report;
select * from ads_user_topic where dt = '2020-08-05';
select * from ads_area_topic where dt = '2020-08-05';
```

7.有问题请看details里面的日志

![image-20200805042815868](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20200805042815868.png)

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/86af1882eb144d59a5b9ba13b7ddf9e5~tplv-k3u1fbpfcp-zoom-1.image)

8.如果跑的中途有错，请按照相关的错误信息修复以后再重新点击Prepare Execution，这时它会从中途错误的job处开始跑，而不是开始的job跑起来。如在第3个job有错停止，查看日志修复后点击Prepare Execution会从第3个job开始跑，而不是从第一个job开始。

![image-20200805043129691](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20200805043129691.png)

再下一步点击执行

![image-20200805043616247](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20200805043616247.png)

## 异常which: no hbase in 

完整信息如下

```sh
which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/java/bin:/opt/module/myscripts/:/opt/module/hd/bin:/opt/module/hd/sbin:/opt/module/zk/bin:/opt/module/fl/bin:/opt/module/kf/bin:/opt/module/sqoop/bin:/opt/module/hive/bin:/opt/module/spark/bin:/opt/module/azkaban/server/bin:/opt/module/azkaban/executor/bin:/home/admin/.local/bin:/home/admin/bin)
```

没有安装hbase，把hbase安装好还是报一样的错。推测可能是profile没有source，在azkaban相关的窗口执行source /etc/profile再执行。

##  异常No files matching path hdfs://hadoop102:..

完整信息

```
No files matching path hdfs://hadoop102:8020/origin_data/gmall/log/topic_start/2020-08-05
```

用户行为数据不存在，请重新采集任务到kakfa再到hdfs。

## 异常JAR does not exist or is not a normal file

hdfs_to_ods_log.job执行报错“JAR does not exist or is not a normal file”，直接在服务器上面执行hdfs_to_ods_log脚本，还是出现“JAR does not exist or is not a normal file”，去网站搜下了一下是hadoop jar的问题，仔细查看语句是这样，jar后面多了个ls。

```
hadoop jar ls $HD_HOM/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2020-05-20
```

修改，把ls去掉

```
hadoop jar $HD_HOM/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2020-05-20
```

## 调试总结

一般脚本出错的概率比较大，azkaban执行到哪个job出错，就把对应的脚本在服务器上执行看是否能正常运行，首先排查脚本内容的错误。

# 可视化报表

按照配置好superset并学会基本使用。

数据库选择gmall_report，表选择ads_user_topic和ads_area_topic；创建一个title为gmall的dashboard仪表盘，增加一个数据源为ads_area_topic表、图表类型为国家地图的图表，坐下面设置选择点击run query并把查询出来的图表保存到gmall仪表盘，图表名为全国各省份订单个数。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/adb9f50fdc3c49b6bc7e6a03c7b346e9~tplv-k3u1fbpfcp-zoom-1.image)

增加一个数据源为ads_area_topic表、图表类型为饼状图的图表，坐下面设置选择点击run query并把查询出来的图表保存到gmall仪表盘，图表名为全国各地订单数量。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/038671ba6f13496c99ef09848b595f00~tplv-k3u1fbpfcp-zoom-1.image)

增加一个数据源为ads_user_topic表、图表类型为走势图/数字和趋势线的图表，坐下面设置选择点击run query并把查询出来的图表保存到gmall仪表盘，图表名为每日活跃用户。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/076e0e3ec26949d9913cf6283acff8fa~tplv-k3u1fbpfcp-zoom-1.image)

设置每隔10秒自动刷新，只要对应表的对应字段有改变，图表内容就会变化。

# 即席查询

kylin基础使用

1.创建一个名称为gmall的工程。

2.获取数据源，点击DataSource，选择所需数据表，并点击Sync按钮。这里选了一张事实表，三张维度表

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3bc73af6fe234e6188948dcdcb602c6c~tplv-k3u1fbpfcp-zoom-1.image)

查看

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c0755985d72d465487d3c96e89896f40~tplv-k3u1fbpfcp-zoom-1.image)

## kylin创建model

1）点击Models，点击"+New"按钮，点击"★New Model"按钮。填写Model名字和描述都为order_detail，点击Next

2）指定事实表为dwd_fact_order_detail，点击Add Lookup Table添加三张维度表dwd_dim_base_province、dwd_dim_sku_info、dwd_dim_user_info_his，对应的连接条件分别是dwd_fact_order_detail表的province_id、sku_id、user_id对应三张表的主键ID。连接方式都是inner join。Tips：多个维度表From Table对应一个事实表是星型模型；多个维度表之间From Table是雪花模型。

以dwd_dim_user_info_his为例：

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/628f436331b44672a56650cd0c0c50e7~tplv-k3u1fbpfcp-zoom-1.image)

完整的效果如下，继续点击next

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2f2cc117062e497ea64886fdb14745d0~tplv-k3u1fbpfcp-zoom-1.image)

3）Dimensions选择维度字段，这里选择的字段都是来自维度表，点击next。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d469ae06aa54978a18aa3d3dac6618f~tplv-k3u1fbpfcp-zoom-1.image)

 4）Measures指定事实表度量字段TOTAL_AMOUNT、SKU_NUM，这里选择的字段都是来自事实表，这里的操作和和上一步类似，点击Next。

5）指定事实表为dwd_fact_order_detail、分区字段（仅支持时间分区）为dt、时间格式为“yyyy-MM-dd”，点击Save按钮，model创建完毕。

## kylin构建cube

1）点击new， 并点击new cube；填写cube信息，选择cube所依赖的model、如刚刚的order_detail，设置cube名字为order_detail_cube，并点击next

2）选择所需的维度，如下图所示

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/33614ae135e84f67a6953bf32fd240e3~tplv-k3u1fbpfcp-zoom-1.image)

3）选择所需度量值，如下图所示，Name值为sum_total_amount

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ae4f8796f55d4a648dc72b9820ed6ed5~tplv-k3u1fbpfcp-zoom-1.image)

4）cube自动合并设置，cube需按照日期分区字段每天进行构建，每次构建的结果会保存在Hbase中的一张表内，为提高查询效率，需将每日的cube进行合并，此处可设置合并周期。

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be4939cf32044befa4aede41d22b4241~tplv-k3u1fbpfcp-zoom-1.image)

5）Kylin高级配置（优化相关，暂时跳过）

6）Kylin相关属性配置覆盖，暂时跳过

7）Cube信息总览，点击Save，Cube创建完成

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e0f310c6a88245e3a99d22d96c23ecba~tplv-k3u1fbpfcp-zoom-1.image)

完成后

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0245e53ff673409da8134cbf7c963aab~tplv-k3u1fbpfcp-zoom-1.image)

8）构建Cube（计算），点击对应Cube的action按钮，选择build，选择要构建的时间区间，点击Submit

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/566942e93dcf48dd9e88de38a70d8b07~tplv-k3u1fbpfcp-zoom-1.image)

9）点击Monitor查看构建进度

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5d4ab829031543a1a6f4ec1ccb43b91b~tplv-k3u1fbpfcp-zoom-1.image)

注意：<font color='red'>必须配置启动Hadoop的jobhistoryserver启动，不然Cube build会报错。</font>

## kylin查询

cube构建完成100%，打开Insight，在new query输入下面sql执行

```sql
select bp.province_name,sum(od.total_amount) from dwd_fact_order_detail od join dwd_dim_base_province bp on od.province_id = bp.id group by bp.province_name
```

查询的语句所涉及的字段、表必须在下图中

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9d441cdbef31449d880abf6d4560e5c4~tplv-k3u1fbpfcp-zoom-1.image)

如图

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/95c8b3881e8d4db3beb4f62e6ea8175d~tplv-k3u1fbpfcp-zoom-1.image)

可以看到查询时间是2秒，没有到达亚秒级，那是因为第一次查询kylin会和hbase构建一些东西，时间会长些，第二次及以后就快了。

## kylin构建异常重复key问题

如果构建出现异常请看log

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d33710b3752044788768ffa28aa42b82~tplv-k3u1fbpfcp-zoom-1.image)

每日全量维度表及拉链维度表重复Key问题如何处理，按照上述流程，会发现，在cube构建流程中出现以下错误

```java
HadoopShellException: java.lang.RuntimeException: Checking snapshot of TableRef[DWD_DIM_SKU_INFO] failed.
...Caused by: java.lang.IllegalStateException: The table: DWD_DIM_SKU_INFO Dup key{重复key] found, key=[1],
```

错误原因分析：上述错误原因是model中的维度表dwd_dim_user_info_his为拉链表、dwd_dim_sku_info为每日全量表，故使用整张表作为维度表，必然会出现订单表中同一个user_id或者sku_id对应多条数据的问题，针对上述问题，有以下两种解决方案。
方案一：在hive中创建维度表的临时表，该临时表中只存放维度表最新的一份完整的数据，在kylin中创建模型时选择该临时表作为维度表。
方案二：与方案一思路相同，但不使用物理临时表，而选用虚拟表、视图（view）实现相同的功能。视图不存储数据，本质是sql查询语句。推荐

（1）创建维度表视图，在hive的gmall数据库里面建立。

```sql
--拉链维度表视图
create view dwd_dim_user_info_his_view as select * from dwd_dim_user_info_his where end_date='9999-99-99';
--全量维度表视图，生产环境执行这个，此处我们是测试环境，不执行
create view dwd_dim_sku_info_view as select * from dwd_dim_sku_info where dt=date_add(current_date,-1);
--当前情形我们先创建一个2020-03-10的视图，执行这个写死日期的
create view dwd_dim_sku_info_view as select * from dwd_dim_sku_info where dt='2020-03-10';
```

（2）在DataSource中导入新创建的视图，导入dwd_dim_user_info_his_view和dwd_dim_sku_info_view，之前的维度表dwd_dim_user_info_his和dwd_dim_sku_info可选择性删除。如果要删除必须执行：

1.先把Monitor里面的job清掉，action里面先选择discard停掉，再选择drop删除掉；

2.在model栏目里把名为order_detail_cube的cube删除掉，把名为order_detail的model也drop掉；

3.再回到database里面把对应的两张表选择unload table清除掉。

（3）重新创建model、cube，并执行查询。之前的操作已经展示过，这里不细说，只要注意对应的表名就行。
（4）查询结果
select
    ui.gender,
    si.category3_id,
    dp.region_id,
    sum(od.total_amount)
from
    dwd_fact_order_detail od
join
    dwd_dim_user_info_view ui
on
    od.user_id=ui.id
join
    dwd_dim_sku_info_view si
on
    od.sku_id=si.id
join
    dwd_dim_base_province dp
on
    od.province_id=dp.id
group by
    ui.gender,si.category3_id,dp.region_id;

2）如何实现每日自动构建cube
Kylin提供了Restful API，因次我们可以将构建cube的命令写到脚本中，将脚本交给azkaban或者oozie这样的调度工具，以实现定时调度的功能。
脚本如下:
#!/bin/bash
cube_name=order_cube
do_date=`date -d '-1 day' +%F`

#获取00:00时间戳
start_date_unix=`date -d "$do_date 08:00:00" +%s`
start_date=$(($start_date_unix*1000))

#获取24:00的时间戳
stop_date=$(($start_date+86400000))

curl -X PUT -H "Authorization: Basic QURNSU46S1lMSU4=" -H 'Content-Type: application/json' -d '{"startTime":'$start_date', "endTime":'$stop_date', "buildType":"BUILD"}' http://hadoop102:7070/kylin/api/cubes/$cube_name/build

## Use RESTful API

类似Java的restful api，以官网的案例

```json
curl -X POST -H "Authorization: Basic XXXXXXXXX" -H "Content-Type: application/json" -d '{ "sql":"select count(*) from TEST_KYLIN_FACT", "project":"learn_kylin" }' http://localhost:7070/kylin/api/query
```

关于Authorization加密，需要把账号秘密（:号隔开）”ADMIN:KYLIN“放到base64加密，百度搜base64加密，如http://tool.chinaz.com/Tools/Base64.aspx。得到类似“QURNSU46S1lMSU4=”的字符串。调整案例如下并放到linux里面执行，需安装好curl

```
curl -X POST -H "Authorization: Basic QURNSU46S1lMSU4=" -H "Content-Type: application/json" -d '{ "sql":"select bp.province_name,sum(od.total_amount) from dwd_fact_order_detail od join dwd_dim_base_province bp on od.province_id = bp.id group by bp.province_name", "project":"gmall" }' http://hadoop102:7070/kylin/api/query
```

执行效果

```html
<!doctype html><html lang="en"><head><title>HTTP Status 401 – Unauthorized</title><style type="text/css">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 {font-size:14px;} p {font-size:12px;} a {color:black;} .line {height:1px;background-color:#525D76;border:none;}</style></head><body><h1>HTTP Status 401 – Unauthorized</h1><hr class="line" /><p><b>Type</b> Status Report</p><p><b>Message</b> Unauthorized</p><p><b>Description</b> The request has not been applied because it lacks valid authentication credentials for the target resource.</p><hr class="line" /><h3>Apache Tomcat/8.5.51</h3></body></html>[admin@hadoop104 ~]$ curl -X POST -H "Authorization: Basic QURNSU46S1lMSU4=" -H "il od join dwd_dim_base_province bp on od.province_id = bp.id group by bp.province_name", "project":"gmall" }' http://hadoop102:7070/kylin/api/query
{"columnMetas":[{"isNullable":1,"displaySize":256,"label":"PROVINCE_NAME","name":"PROVINCE_NAME","schemaName":"GMALL","catelogName":null,"tableName":"DWD_DIM_BASE_PROVINCE","precision":256,"scale":0,"columnType":12,"columnTypeName":"VARCHAR","readOnly":true,"autoIncrement":false,"caseSensitive":true,"searchable":false,"currency":false,"definitelyWritable":false,"writable":false,"signed":true},{"isNullable":1,"displaySize":19,"label":"EXPR$1","name":"EXPR$1","schemaName":null,"catelogName":null,"tableName":null,"precision":19,"scale":2,"columnType":3,"columnTypeName":"DECIMAL","readOnly":true,"autoIncrement":false,"caseSensitive":true,"searchable":false,"currency":false,"definitelyWritable":false,"writable":false,"signed":true}],"results":[["山东","488"],["黑龙江","1553"],["西藏","488"],["福建","290"],["四川","222"],["贵州","3106"]],"cube":"CUBE[name=order_detail_cube]","affectedRowCount":0,"isException":false,"exceptionMessage":null,"duration":70,"totalScanCount":6,"totalScanBytes":258,"hitExceptionCache":false,"storageCacheUsed":false,"traceUrl":null,"pushDown":false,"partial":false}
```

把其中的json格式化，其中results下是查询的结果

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c91c80dd9bb4609aff085d6ba151431~tplv-k3u1fbpfcp-zoom-1.image)

再以官网的build cube为例

```
curl -X PUT -H "Authorization: Basic XXXXXXXXX" -H 'Content-Type: application/json' -d '{"startTime":'1423526400000', "endTime":'1423612800000', "buildType":"BUILD"}' http://<host>:<port>/kylin/api/cubes/{cubeName}/build
```

调整后，时间需要自己转换

```
curl -X PUT -H "Authorization: Basic QURNSU46S1lMSU4=" -H 'Content-Type: application/json' -d '{"startTime":'1423526400000', "endTime":'1423612800000', "buildType":"BUILD"}' http://<host>:<port>/kylin/api/cubes/order_detail_cube/build
```

实现每日自动构建cube
Kylin提供了Restful API，因次我们可以将构建cube的命令写到脚本中，将脚本交给azkaban或者oozie这样的调度工具，以实现定时调度的功能。具体生产环境可以执行cube_build每日自动构建cube脚本

## presto使用

```
xcall $PRESTO_HOME/bin/launcher start #保证Presto启动
/opt/module/yanagishima/bin/yanagishima-start.sh
# 启动web页面http://hadoop102:7080 
```

就是一个简单业务，hive和mysql因为在presto catalog里面配置了hive和mysql

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3d9968d838a14803a609945de0803a9c~tplv-k3u1fbpfcp-zoom-1.image)


# 集群监控





# 1