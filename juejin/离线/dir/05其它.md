# 全流程调度

建表

```
mysql -uroot -p1234 -e "CREATE DATABASE gmall_report CHARACTER SET utf8 COLLATE utf8_general_ci;"
# 可以把ods层所有的表都在gmall_report数据库创建，把ods_sql里面的建表语句全部拿出来即可，这里只以创建用户主题表和地区主题表为例。
DROP TABLE IF EXISTS `ads_user_topic`;
CREATE TABLE `ads_user_topic`  (
  `dt` date NOT NULL,
  `day_users` bigint(255) NULL DEFAULT NULL,
  `day_new_users` bigint(255) NULL DEFAULT NULL,
  `day_new_payment_users` bigint(255) NULL DEFAULT NULL,
  `payment_users` bigint(255) NULL DEFAULT NULL,
  `users` bigint(255) NULL DEFAULT NULL,
  `day_users2users` double(255, 2) NULL DEFAULT NULL,
  `payment_users2users` double(255, 2) NULL DEFAULT NULL,
  `day_new_users2users` double(255, 2) NULL DEFAULT NULL,
  PRIMARY KEY (`dt`) USING BTREE
) COMMENT='用户主题表' ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
DROP TABLE IF EXISTS `ads_area_topic`;
CREATE TABLE `ads_area_topic`  (
  `dt` date NOT NULL,
  `id` int(11) NULL DEFAULT NULL,
  `province_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `area_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `iso_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `region_id` int(11) NULL DEFAULT NULL,
  `region_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `order_day_count` bigint(255) NULL DEFAULT NULL,
  `order_day_amount` double(255, 2) NULL DEFAULT NULL,
  `payment_day_count` bigint(255) NULL DEFAULT NULL,
  `payment_day_amount` double(255, 2) NULL DEFAULT NULL,
  PRIMARY KEY (`dt`, `iso_code`) USING BTREE
)COMMENT='地区主题表'  ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
```

hdfs_to_mysql导出Sqoop脚本的简单说明：关于导出update还是insert的问题。
--update-mode：updateonly只更新不新增、allowinsert允许新增 。
--update-key：允许更新的情况下，指定哪些字段匹配视为同一条数据，进行更新而不增加。多个字段用逗号分隔。
--input-null-string和--input-null-non-string：分别表示，将字符串列和非字符串列的空串和“null”转义。
官网地址：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html
Sqoop will by default import NULL values as string null. Hive is however using string \N to denote NULL values and therefore predicates dealing with NULL(like IS NULL) will not work correctly. You should append parameters --null-string and --null-non-string in case of import job or --input-null-string and --input-null-non-string in case of an export job if you wish to properly preserve NULL values. Because sqoop is using those parameters in generated code, you need to properly escape value \N to \\N:
Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。

```
hdfs_to_mysql all # hdfs_to_mysql导出Sqoop脚本，事先启动hadoop集群
select * from ads_user_topic;
select * from ads_area_topic;
```

## 全调度流程涉及脚本

关系展示

| 层级 | 名称                 | 简介                     | 数据类型类型           | 依赖脚本                                                  |
| ---- | -------------------- | ------------------------ | ---------------------- | --------------------------------------------------------- |
| 采集 | gmall_mysql_to_hdfs  | mysql->HDFS，sqoop脚本   | 业务数据               |                                                           |
| ods  | hdfs_to_ods_log      | hive脚本                 | 用户行为数据           |                                                           |
| ods  | hdfs_to_ods_db       | hive脚本                 | 业务数据               | gmall_mysql_to_hdfs                                       |
| dwd  | ods_to_dwd_start_log | 启动表，hive脚本         | 用户行为数据           | hdfs_to_ods_log                                           |
| dwd  | ods_to_dwd_base_log  | 事件日志基础表，hive脚本 | 用户行为数据           | hdfs_to_ods_log                                           |
| dwd  | dwd_events_log       | 事件日志具体表，hive脚本 | 用户行为数据           | ods_to_dwd_start_log                                      |
| dwd  | ods_to_dwd_db        | hive脚本                 | 业务数据               | hdfs_to_ods_db                                            |
| dws  | dwd_to_dws           | hive脚本                 | 用户行为数据和业务数据 | ods_to_dwd_start_log、dwd_events_log(可略)、ods_to_dwd_db |
| dwt  | dws_to_dwt           | hive脚本                 | 用户行为数据和业务数据 | dwd_to_dws                                                |
| ads  | dwt_to_ads           | hive脚本                 | 用户行为数据和业务数据 | dws_to_dwt                                                |
| 调度 | hdfs_to_mysql        | HDFS->mysql，sqoop脚本   | 用户行为数据和业务数据 | dwt_to_ads                                                |

**注意**：层级分为采集=>ods=>dwd=>dws=>dwt=>ads=>调度六个层级；ods_to_dwd_base_log和dwd_events_log这两个脚本在调度全流程中可省略。把下面九个后缀为job的文件全部放在gmall目录，打包为gmall.zip，因为目前azkaban内置的任务类型支持的command中，Azkaban上传的工作流文件只支持xxx.zip文件。zip应包含xxx.job运行作业所需的文件和任何文件（文件名后缀必须以.job结尾，否则无法识别）。作业名称在项目中必须是唯一的。下面Job的配置请结合上面的关系展示表格理解，脚本的路径请改为自己的路径。最后：关于省略的脚本ods_to_dwd_base_log和dwd_events_log想加的话也可以加在job里。

（1）gmall_mysql_to_hdfs.job

```sh
type=command
/opt/module/myscripts/gmall_mysql_to_hdfs all ${dt}
```

（2）hdfs_to_ods_log.job

```sh
type=command
/opt/module/myscripts/hdfs_to_ods_log ${dt}
```

（3）hdfs_to_ods_db.job，注意dependencies依赖

```sh
type=command
/opt/module/myscripts/hdfs_to_ods_db all ${dt}
dependencies=gmall_mysql_to_hdfs
```

（4）ods_to_dwd_start_log.job

```sh
type=command
/opt/module/myscripts/ods_to_dwd_start_log ${dt}
dependencies=hdfs_to_ods_log
```

（5）ods_to_dwd_db.job

```
type=command
/opt/module/myscripts/ods_to_dwd_db all ${dt}
dependencies=hdfs_to_ods_db
```

（6）dwd_to_dws.job 依赖有多个用逗号分开，dwd_events_log略

```sh
type=command
/opt/module/myscripts/dwd_to_dws ${dt}
dependencies=ods_to_dwd_db,ods_to_dwd_start_log
```

（7）dws_to_dwt.job

```sh
type=command
/opt/module/myscripts/dws_to_dwt ${dt}
dependencies=dwd_to_dws
```

（8）dwt_to_ads.job

```sh
type=command
/opt/module/myscripts/dwt_to_ads ${dt}
dependencies=dws_to_dwt
```

（9）hdfs_to_mysql.job

```sh
type=command
/opt/module/myscripts/hdfs_to_mysql all
dependencies=dwt_to_ads
```

## 调度开始

0.生成对应日期的业务数据和用户行为数据

```
# 用户行为数据 确保hd zk kf启动
lg # hadoop102和hadoop103的/tmp/logs/生成日志
f1 start # 把hadoop102和hadoop103的日志采集过滤并放到kafka主题topic_start、topic_event里，确保flume的lib下有日志过滤flume-interceptor生成的jar包
# 启动两个消费者分别消费kafka上的topic_start、topic_event里的日志数据，存到hdfs里面。
flume-ng agent -f $FL_HOME/conf/kafka-flume-hdfs.conf -n a1 -Dflume.root.logger=INFO,LOGFILE 
# 确保hdfs下/origin_data/gmall/log/topic_start或topic_event下有后缀为lzo的数据
# 业务数据，在hadoop104执行。确保hadoop102的/opt/module/db_log下gmall-mock-db.jar
vim /opt/module/db_log/application.properties# 修改文件里的mock.date为当前日期，和生成日志的日期一致

jar -jar /opt/module/db_log/gmall-mock-db.jar
mysql -uroot -p1234 -e "use gmall;select * from user_info where create_time = 'mock.date日期' limit 1;"
```

1.启动azkaban-executor-start.sh和azkaban-web-start.sh，访问https://hadoop102:8443/（注意https）输入admin/admin登录；

2.azkaban web客户端中Projects栏目下创建项目，点击Create Project，填写Name为gmall；Description为gmall；

3.Projects栏目下点击创建好的项目gmall进去，上传压缩好的gmall.zip到gmall项目。azkaban会自动校验zip包是否有问题；

4.点击 Execute Flow会出现下图，展示脚本之间的依赖关系，schedule定时调度、execute马上执行，生产环境选择schedule定时调度，测试环境选择execute马上执行。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff09c8ad58de4e929948eb695a038aa4~tplv-k3u1fbpfcp-zoom-1.image)

5.job文件里面有${dt}，Flow Parameters里面添加add rows，名称为dt。如果是生产环境执行，dt无需设置值，如果是测试，dt必须设置值，如值为2020-03-11。设置好dt

```sql
use gmall_report;
delete from ads_user_topic where dt = '2020-08-05';
delete from ads_area_topic where dt = '2020-08-05';
```

设置好以后点击execute再点击continue执行，执行完成大概需要20-60分钟。

6.执行完毕，最后hdfs_to_mysql的job变为绿色表示全部流程执行成功。

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0652ddace9b6401cb21cc1a0b26b8f9b~tplv-k3u1fbpfcp-zoom-1.image)

执行列表

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b46e506683e44e5c89810767d08e4188~tplv-k3u1fbpfcp-zoom-1.image)

查询

```sql
use gmall_report;
select * from ads_user_topic where dt = '2020-08-05';
select * from ads_area_topic where dt = '2020-08-05';
```

7.有问题请看details里面的日志

![image-20200805042815868](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20200805042815868.png)

![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/86af1882eb144d59a5b9ba13b7ddf9e5~tplv-k3u1fbpfcp-zoom-1.image)

8.如果跑的中途有错，请按照相关的错误信息修复以后再重新点击Prepare Execution，这时它会从中途错误的job处开始跑，而不是开始的job跑起来。如在第3个job有错停止，查看日志修复后点击Prepare Execution会从第3个job开始跑，而不是从第一个job开始。

![image-20200805043129691](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20200805043129691.png)

再下一步点击执行

![image-20200805043616247](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20200805043616247.png)

## 异常which: no hbase in 

完整信息如下

```sh
which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/java/bin:/opt/module/myscripts/:/opt/module/hd/bin:/opt/module/hd/sbin:/opt/module/zk/bin:/opt/module/fl/bin:/opt/module/kf/bin:/opt/module/sqoop/bin:/opt/module/hive/bin:/opt/module/spark/bin:/opt/module/azkaban/server/bin:/opt/module/azkaban/executor/bin:/home/admin/.local/bin:/home/admin/bin)
```

没有安装hbase，把hbase安装好还是报一样的错。推测可能是profile没有source，在azkaban相关的窗口执行source /etc/profile再执行。

##  异常No files matching path hdfs://hadoop102:..

完整信息

```
No files matching path hdfs://hadoop102:8020/origin_data/gmall/log/topic_start/2020-08-05
```

用户行为数据不存在，请重新采集任务到kakfa再到hdfs。

## 异常JAR does not exist or is not a normal file

hdfs_to_ods_log.job执行报错“JAR does not exist or is not a normal file”，直接在服务器上面执行hdfs_to_ods_log脚本，还是出现“JAR does not exist or is not a normal file”，去网站搜下了一下是hadoop jar的问题，仔细查看语句是这样，jar后面多了个ls。

```
hadoop jar ls $HD_HOM/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2020-05-20
```

修改，把ls去掉

```
hadoop jar $HD_HOM/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2020-05-20
```

## 调试总结

一般脚本出错的概率比较大，azkaban执行到哪个job出错，就把对应的脚本在服务器上执行看是否能正常运行，首先排查脚本内容的错误。

# 可视化报表

按照配置好superset并学会基本使用。

数据库选择gmall_report，表选择ads_user_topic和ads_area_topic；创建一个title为gmall的dashboard仪表盘，增加一个数据源为ads_area_topic表、图表类型为国家地图的图表，坐下面设置选择点击run query并把查询出来的图表保存到gmall仪表盘，图表名为全国各省份订单个数。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/adb9f50fdc3c49b6bc7e6a03c7b346e9~tplv-k3u1fbpfcp-zoom-1.image)

增加一个数据源为ads_area_topic表、图表类型为饼状图的图表，坐下面设置选择点击run query并把查询出来的图表保存到gmall仪表盘，图表名为全国各地订单数量。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/038671ba6f13496c99ef09848b595f00~tplv-k3u1fbpfcp-zoom-1.image)

增加一个数据源为ads_user_topic表、图表类型为走势图/数字和趋势线的图表，坐下面设置选择点击run query并把查询出来的图表保存到gmall仪表盘，图表名为每日活跃用户。

![](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/076e0e3ec26949d9913cf6283acff8fa~tplv-k3u1fbpfcp-zoom-1.image)



# 即席查询



# 集群监控



# 1