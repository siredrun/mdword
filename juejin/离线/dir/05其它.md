# 全流程调度

建表

```
mysql -uroot -p1234 -e "CREATE DATABASE gmall_report CHARACTER SET utf8 COLLATE utf8_general_ci;"
# 可以把ods层所有的表都在gmall_report数据库创建，把ods_sql里面的建表语句全部拿出来即可，这里只以创建用户主题表和地区主题表为例。
DROP TABLE IF EXISTS `ads_user_topic`;
CREATE TABLE `ads_user_topic`  (
  `dt` date NOT NULL,
  `day_users` bigint(255) NULL DEFAULT NULL,
  `day_new_users` bigint(255) NULL DEFAULT NULL,
  `day_new_payment_users` bigint(255) NULL DEFAULT NULL,
  `payment_users` bigint(255) NULL DEFAULT NULL,
  `users` bigint(255) NULL DEFAULT NULL,
  `day_users2users` double(255, 2) NULL DEFAULT NULL,
  `payment_users2users` double(255, 2) NULL DEFAULT NULL,
  `day_new_users2users` double(255, 2) NULL DEFAULT NULL,
  PRIMARY KEY (`dt`) USING BTREE
) COMMENT='用户主题表' ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
DROP TABLE IF EXISTS `ads_area_topic`;
CREATE TABLE `ads_area_topic`  (
  `dt` date NOT NULL,
  `id` int(11) NULL DEFAULT NULL,
  `province_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `area_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `iso_code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `region_id` int(11) NULL DEFAULT NULL,
  `region_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `order_day_count` bigint(255) NULL DEFAULT NULL,
  `order_day_amount` double(255, 2) NULL DEFAULT NULL,
  `payment_day_count` bigint(255) NULL DEFAULT NULL,
  `payment_day_amount` double(255, 2) NULL DEFAULT NULL,
  PRIMARY KEY (`dt`, `iso_code`) USING BTREE
)COMMENT='地区主题表'  ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;
```

hdfs_to_mysql导出Sqoop脚本的简单说明：关于导出update还是insert的问题。
--update-mode：updateonly只更新不新增、allowinsert允许新增 。
--update-key：允许更新的情况下，指定哪些字段匹配视为同一条数据，进行更新而不增加。多个字段用逗号分隔。
--input-null-string和--input-null-non-string：分别表示，将字符串列和非字符串列的空串和“null”转义。
官网地址：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html
Sqoop will by default import NULL values as string null. Hive is however using string \N to denote NULL values and therefore predicates dealing with NULL(like IS NULL) will not work correctly. You should append parameters --null-string and --null-non-string in case of import job or --input-null-string and --input-null-non-string in case of an export job if you wish to properly preserve NULL values. Because sqoop is using those parameters in generated code, you need to properly escape value \N to \\N:
Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。

```
hdfs_to_mysql all # hdfs_to_mysql导出Sqoop脚本，事先启动hadoop集群
select * from ads_user_topic;
select * from ads_area_topic;
```

## 全调度流程涉及脚本

关系展示

| 层级 | 名称                 | 简介                     | 数据类型类型           | 依赖脚本                                                  |
| ---- | -------------------- | ------------------------ | ---------------------- | --------------------------------------------------------- |
| 采集 | gmall_mysql_to_hdfs  | mysql->HDFS，sqoop脚本   | 业务数据               |                                                           |
| ods  | hdfs_to_ods_log      | hive脚本                 | 用户行为数据           |                                                           |
| ods  | hdfs_to_ods_db       | hive脚本                 | 业务数据               | gmall_mysql_to_hdfs                                       |
| dwd  | ods_to_dwd_start_log | 启动表，hive脚本         | 用户行为数据           | hdfs_to_ods_log                                           |
| dwd  | ods_to_dwd_base_log  | 事件日志基础表，hive脚本 | 用户行为数据           | hdfs_to_ods_log                                           |
| dwd  | dwd_events_log       | 事件日志具体表，hive脚本 | 用户行为数据           | ods_to_dwd_start_log                                      |
| dwd  | ods_to_dwd_db        | hive脚本                 | 业务数据               | hdfs_to_ods_db                                            |
| dws  | dwd_to_dws           | hive脚本                 | 用户行为数据和业务数据 | ods_to_dwd_start_log、dwd_events_log(可略)、ods_to_dwd_db |
| dwt  | dws_to_dwt           | hive脚本                 | 用户行为数据和业务数据 | dwd_to_dws                                                |
| ads  | dwt_to_ads           | hive脚本                 | 用户行为数据和业务数据 | dws_to_dwt                                                |
| 调度 | hdfs_to_mysql        | HDFS->mysql，sqoop脚本   | 用户行为数据和业务数据 | dwt_to_ads                                                |

**注意**：层级分为采集=>ods=>dwd=>dws=>dwt=>ads=>调度六个层级；ods_to_dwd_base_log和dwd_events_log这两个脚本在调度全流程中可省略。把下面九个后缀为job的文件全部放在gmall目录，打包为gmall.zip，因为目前azkaban内置的任务类型支持的command中，Azkaban上传的工作流文件只支持xxx.zip文件。zip应包含xxx.job运行作业所需的文件和任何文件（文件名后缀必须以.job结尾，否则无法识别）。作业名称在项目中必须是唯一的。下面Job的配置请结合上面的关系展示表格理解，脚本的路径请改为自己的路径。最后：关于省略的脚本ods_to_dwd_base_log和dwd_events_log想加的话也可以加在job里。

（1）gmall_mysql_to_hdfs.job

```sh
type=command
/opt/module/myscripts/gmall_mysql_to_hdfs.sh all ${dt}
```

（2）hdfs_to_ods_log.job

```sh
type=command
/opt/module/myscripts/hdfs_to_ods_log.sh ${dt}
```

（3）hdfs_to_ods_db.job，注意dependencies依赖

```sh
type=command
/opt/module/myscripts/hdfs_to_ods_db.sh all ${dt}
dependencies=gmall_mysql_to_hdfs
```

（4）ods_to_dwd_start_log.job

```sh
type=command
/opt/module/myscripts/ods_to_dwd_start_log.sh ${dt}
dependencies=hdfs_to_ods_log
```

（5）ods_to_dwd_db.job

```
type=command
/opt/module/myscripts/ods_to_dwd_db.sh all ${dt}
dependencies=hdfs_to_ods_db
```

（6）dwd_to_dws.job 依赖有多个用逗号分开，dwd_events_log略

```sh
type=command
/opt/module/myscripts/dwd_to_dws.sh ${dt}
dependencies=ods_to_dwd_db,ods_to_dwd_start_log
```

（7）dws_to_dwt.job

```sh
type=command
/opt/module/myscripts/dws_to_dwt ${dt}
dependencies=dwd_to_dws
```

（8）dwt_to_ads.job

```sh
type=command
/opt/module/myscripts/dwt_to_ads ${dt}
dependencies=dws_to_dwt
```

（9）hdfs_to_mysql.job

```sh
type=command
/opt/module/myscripts/hdfs_to_mysql.sh all
dependencies=dwt_to_ads
```

## 调度开始

0.生成对应日期的业务数据和用户行为数据

```
# 用户行为数据 确保hd zk kf启动
lg # hadoop102和hadoop103的/tmp/logs/生成日志
f1 start # 把hadoop102和hadoop103的日志采集过滤并放到kafka主题topic_start、topic_event里，确保flume的lib下有日志过滤flume-interceptor生成的jar包
# 启动两个消费者分别消费kafka上的topic_start、topic_event里的日志数据，存到hdfs里面。
flume-ng agent -f $FL_HOME/conf/kafka-flume-hdfs.conf -n a1 -Dflume.root.logger=INFO,LOGFILE 
# 确保hdfs下/origin_data/gmall/log/topic_start或topic_event下有后缀为lzo的数据
# 业务数据，在hadoop104执行。确保hadoop102的/opt/module/db_log下gmall-mock-db.jar
vim /opt/module/db_log/application.properties# 修改文件里的mock.date为当前日期，和生成日志的日期一致

jar -jar /opt/module/db_log/gmall-mock-db.jar
mysql -uroot -p1234 -e "use gmall;select * from user_info where create_time = 'mock.date日期' limit 1;"
```

1.启动azkaban-executor-start.sh和azkaban-web-start.sh，访问https://hadoop102:8443/（注意https）输入admin/admin登录；

2.azkaban web客户端中Projects栏目下创建项目，点击Create Project，填写Name为gmall；Description为gmall；

3.Projects栏目下点击创建好的项目gmall进去，上传压缩好的gmall.zip到gmall项目。azkaban会自动校验zip包是否有问题；

4.点击 Execute Flow会出现下图，展示脚本之间的依赖关系，schedule定时调度、execute马上执行，生产环境选择schedule定时调度，测试环境选择execute马上执行。

![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff09c8ad58de4e929948eb695a038aa4~tplv-k3u1fbpfcp-zoom-1.image)

5.job文件里面有${dt}，Flow Parameters里面添加add rows，名称为dt。如果是生产环境执行，dt无需设置值，如果是测试，dt必须设置值，如值为2020-03-11。设置好dt

```sql
delete from ads_user_topic where dt = '2020-03-11';
delete from ads_area_topic where dt = '2020-03-11';
```



设置好以后点击execute执行，执行完成大概需要20-60分钟。

6.执行前

# 可视化报表



# 即席查询



# 集群监控



# 1