# 概述

对于shell脚本不太熟的建议看看这个：[shell基础教程](https://juejin.im/post/5e6c29bb518825493038df24)  。

项目中使用到许多脚本和一些配置文件，为了方便管理全部都在此。在服务器中为方便执行，全部都在/opt/module/myscripts下，如使用root账户，就放在/root/bin下。脚本执行的基本三要素：hostname、hosts、ssh免密登录。

脚本文件目录

```
xcall集群整体操作脚本
xsync同步脚本
xsync2同步脚本（除主节点外）
jpsall集群Java实例查看脚本
ct集群时间更新 更新到现在时间
dt集群时间修改
lg日志生成脚本
hd群启hadoop脚本
zk群启zookeeper脚本
kf群启Kafka脚本
file-flume-kafka.conf日志采集Flume Agent
kafka-flume-hdfs.conf日志消费Flume Agent
f1日志采集Flume启动停止脚本
f2日志消费Flume启动停止脚本
keepalived.conf高可用MysqlHA
gmall_mysql_to_hdfs业务数据导入HDFS
hdfs_to_ods_log用户行为数据加载脚本ODS层
```

关于脚本概述

```
mkdir -p /opt/module/myscripts
vim /etc/profile # 加入下面内容
#SCRIPT_HOME
export SCRIPT_HOME=/opt/module/myscripts
export PATH=$PATH:$SCRIPT_HOME

xsync /etc/profile
source /etc/profile
xsync $SCRIPT_HOME
# 所有脚本（有!#/bin/bash）都要执行下面三步操作，以同步脚本xsync为例
vim $SCRIPT_HOME/xsync # 建立文件，输入内容
chmod +x $SCRIPT_HOME/xsync # 增加权限
xsync $SCRIPT_HOME/xsync # 同步到所有集群
```

# xcall集群整体操作脚本

```sh
#!/bin/bash
#验证参数
if(($#==0))
then
        echo 请传入要执行的命令!
        exit;
fi

echo "要执行的命令是:$*"

#批量执行
for i in hadoop102 hadoop103 hadoop104
do
        echo -----------------------$i---------------------
        echo $i $*
        ssh  $i $*
done
```

# xsync同步脚本

```sh
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for i in hadoop102 hadoop103 hadoop104
do
        echo ------------------- $i --------------
        rsync -av $pdir/$fname $user@$i:$pdir
done
```

# xsync2同步脚本（除主节点外）

```sh
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for i in hadoop103 hadoop104
do
        echo ------------------- $i --------------
        rsync -av $pdir/$fname $user@$i:$pdir
done
```

# jpsall集群Java实例查看脚本

```
#!/bin/bash
for i in hadoop102 hadoop103 hadoop104
do
echo "========== $i ==========" 
ssh $i $JAVA_HOME/bin/jps
done
```

# ct集群时间更新

```sh
#!/bin/bash
#将集群的时间都同步为最新的时间
for i in hadoop102 hadoop103 hadoop104
do
	echo -----------$i---------------
	ssh $i ntpdate -u ntp1.aliyun.com
done
```

# dt集群时间修改

使用 dt '2019-20-35'

```sh
#/bin/bash
#在hadoop102、hadoop103、hadoop104上同步日期为指定的日期
if(($#==0))
then
	echo 请输入要修改的时间!
	exit;
fi

#修改系统时间
for i in hadoop102 hadoop103 hadoop104
do
	echo ------------同步$i时间--------------
	ssh $i "sudo date -s '$@'"
done
```

# lg日志生成脚本

说明：直接执行lg会在hadoop102和hadoop103分别产生数据。

```sh
#!/bin/bash
#在hadoop102,hadoop103产生日志
for i in hadoop102 hadoop103
do
    ssh $i $JAVA_HOME/bin/java -jar /opt/module/logcollector-1.0-SNAPSHOT-jar-with-dependencies.jar >/dev/null 2>&1	
done
```

# hd群启hadoop脚本

```sh
#!/bin/bash
#hadoop集群的一键启动脚本
if(($#!=1))
then
	echo '请输入start|stop|restart参数!'
	exit;
fi

#只允许传入start和stop参数
if [ $1 = start ] || [ $1 = stop ]
then
	ssh hadoop102 /opt/module/hd/sbin/$1-dfs.sh
	ssh hadoop103 /opt/module/hd/sbin/$1-yarn.sh
elif [ $1 = restart ]
then
	echo ---------------stop---------------
        ssh hadoop102 /opt/module/hd/sbin/stop-dfs.sh
        ssh hadoop103 /opt/module/hd/sbin/stop-yarn.sh

	echo ---------------start---------------
        ssh hadoop102 /opt/module/hd/sbin/start-dfs.sh
        ssh hadoop103 /opt/module/hd/sbin/start-yarn.sh
else
	echo '请输入start|stop参数!'
fi
```



# zk群启zookeeper脚本

```sh
#!/bin/bash
#hadoop集群的一键启动脚本
if(($#!=1))
then
        echo '请输入start|stop|status|restart!'
        exit;
fi

if [ $1 = start ] || [ $1 = stop ]
then
        echo -----------$1---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh $1
        done
        echo -----------status---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh status
        done
elif [ $1 = status ]
then
        echo -----------status---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh status
        done
elif [ $1 = restart ]
then
        echo -----------stop---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh stop
        done
        echo -----------start---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh start
        done
        echo -----------status---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh status
        done
else
        echo '请输入start|stop|status|restart!'
fi
```

# kf群启Kafka脚本

```sh
#!/bin/bash
#只接收start和stop参数
if(($#!=1))
then
	echo '请输入start|stop|restart!'
	exit;
fi

if [ $1 = start ]
then
	echo -----------start---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-start.sh -daemon $KF_HOME/config/server.properties
	done
elif [ $1 = stop ]
then
	echo -----------stop---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-stop.sh
	done
elif [ $1 = restart ]
then
	echo -----------stop---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-stop.sh
	done
	echo -----------start---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-start.sh -daemon $KF_HOME/config/server.properties
	done
else
	echo '请输入start|stop|restart!'
fi
```

# file-flume-kafka.conf日志采集Flume Agent

Flume Agent配置文件*.conf都是放在$FL_HOME/conf下的vim $FL_HOME/conf/file-flume-kafka.conf.conf。
注意：com.demo.flume.interceptor.LogETLInterceptor和com.demo.flume.interceptor.LogTypeInterceptor是自定义的拦截器的全类名。需要根据用户自定义的拦截器做相应修改。

```
a1.sources=r1
a1.channels=c1 c2

# configure source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/fl/test/log_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
a1.sources.r1.fileHeader = true
a1.sources.r1.channels = c1 c2

#interceptor
a1.sources.r1.interceptors =  i1 i2
a1.sources.r1.interceptors.i1.type = com.demo.flume.interceptor.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com.demo.flume.interceptor.LogTypeInterceptor$Builder

a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.parseAsFlumeEvent = false
a1.channels.c1.kafka.consumer.group.id = flume-consumer

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false
a1.channels.c2.kafka.consumer.group.id = flume-consumer
```

# kafka-flume-hdfs.conf日志消费Flume Agent

Flume Agent配置文件*.conf都是放在$FL_HOME/conf下的vim $FL_HOME/conf/kafka-flume-hdfs.conf。

```
## 组件
a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_start

## source2
a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000
a1.sources.r2.batchDurationMillis = 2000
a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics=topic_event

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/f1/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/f1/data/behavior1/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6

## channel2
a1.channels.c2.type = file
a1.channels.c2.checkpointDir = /opt/module/f1/checkpoint/behavior2
a1.channels.c2.dataDirs = /opt/module/f1/data/behavior2/
a1.channels.c2.maxFileSize = 2146435071
a1.channels.c2.capacity = 1000000
a1.channels.c2.keep-alive = 6

## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-

##sink2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-

## 不要产生大量小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream 
a1.sinks.k2.hdfs.fileType = CompressedStream 

a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k2.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel= c2
```



# f1日志采集Flume启动停止脚本

```sh
#! /bin/bash

case $1 in
"start"){
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup $FL_HOME/bin/flume-ng agent -f $FL_HOME/conf/file-flume-kafka.conf -n a1 -Dflume.root.logger=DEBUG,console > $FL_HOME/f1 2>&1 &"
        done
};;	
"stop"){
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac
```

# f2日志消费Flume启动停止脚本

```bash
#! /bin/bash

case $1 in
"start"){
        for i in hadoop104
        do
                echo " --------启动 $i 消费flume-------"
                ssh $i "nohup $FL_HOME/bin/flume-ng agent -f $FL_HOME/conf/kafka-flume-hdfs.conf -n a1 -Dflume.root.logger=INFO,LOGFILE > /$FL_HOME/f2 2>&1 &"
        done
};;
"stop"){
        for i in hadoop104
        do
                echo " --------停止 $i 消费flume-------"
                ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        done

};;
esac
```

# keepalived.conf高可用MysqlHA

路径：/etc/keepalived/keepalived.conf

```
! Configuration File for keepalived
global_defs {
   router_id MySQL-ha
}
vrrp_instance VI_1 {
    state master # 初始状态
    interface eth0 # 网卡
    virtual_router_id 51 # 虚拟器路由ID
    priority 100 # 优先级
    advert_int 1 # Keepalived心跳间隔
    nopreempt # 只在高优先级配置， 原master恢复之后不重新上位
    authentication {
        auth_type PASS # 认证相关
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.200.100 # 虚拟ip
    }
}
#声明虚拟服务器
virtual_server 192.168.200.100 3306 {
    delay_loop 6
    persistence_timeout 30
    protocol TCP
	# 声明真实服务器
    real_server 172.31.82.15 3306 {
    	notify_down /var/lib/mysql/killkeepalived.sh # 真实服务故障后调用脚本
    	TCP_CHECK {
    		connect_timeout 3 # 超时时间
    		nb_get_retry 1 # 重试次数
    		delay_before_retry 1 # 重试时间间隔
    	}
    }
}
```

# gmall_mysql_to_hdfs业务数据导入HDFS

脚本参数说明：frist包含所有的表，类似全量同步策略；all除开base_province省份表和base_region地区表的所有导入、类似新增同步策略；后面跟上具体表名：gmall_mysql_to_hdfs order_info，导入order_info的数据。Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。

```sh
#! /bin/bash

sqoop=/opt/module/sqoop/bin/sqoop
do_date=`date -d '-1 day' +%F`

if [[ -n "$2" ]]; then
    do_date=$2
fi

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 1234 \
--target-dir /origin_data/gmall/db/$1/$do_date \
--delete-target-dir \
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hd/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/gmall/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            final_total_amount, 
                            order_status, 
                            user_id, 
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            province_id,
                            benefit_reduce_amount,
                            original_total_amount,
                            feight_fee      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date')"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

import_activity_order(){
  import_data activity_order "select
                                id,
                                activity_id,
                                order_id,
                                create_time
                              from activity_order
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            name,
                            birthday,
                            gender,
                            email,
                            user_level, 
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
  import_data order_detail "select 
                              od.id,
                              order_id, 
                              user_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              od.create_time  
                            from order_detail od
                            join order_info oi
                            on od.order_id=oi.id
                            where DATE_FORMAT(od.create_time,'%Y-%m-%d')='$do_date'"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                alipay_trade_no, 
                                total_amount,  
                                subject, 
                                payment_type, 
                                payment_time 
                              from payment_info 
                              where DATE_FORMAT(payment_time,'%Y-%m-%d')='$do_date'"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              comment_txt,
                              create_time
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                create_time
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          create_time
                        from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3 where 1=1"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code
                            from base_province
                            where 1=1"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region
                            where 1=1"
}

import_base_trademark(){
  import_data base_trademark "select
                                tm_id,
                                tm_name
                              from base_trademark
                              where 1=1"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info
                          where 1=1"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info
                        where 1=1"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time
                      from cart_info
                      where 1=1"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          spu_id,
                          tm_id,
                          category3_id,
                          limit_num,
                          operate_time,
                          expire_time
                        from coupon_info
                        where 1=1"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info
                            where 1=1"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule
                                where 1=1"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic
                          where 1=1" 
}

case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "activity_order")
      import_activity_order
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;

"first")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_province
   import_base_region
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
"all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
esac
```

# hdfs_to_ods_log用户行为数据加载脚本ODS层

只能在有安装hive的节点上执行，执行：ods_log 2020-05-20。

```sh
#!/bin/bash

db=gmall
hive=/opt/module/hive/bin/hive
do_date=`date -d '-1 day' +%F`

if [[ -n "$1" ]]; then
    do_date=$1
fi

sql="
load data inpath '/origin_data/gmall/log/topic_start/$do_date' into table ${db}.ods_start_log partition(dt='$do_date');
load data inpath '/origin_data/gmall/log/topic_event/$do_date' into table ${db}.ods_event_log partition(dt='$do_date');
"

$hive -e "$sql"
hadoop jar /opt/module/hd/share/hadoop/common/hadoop-lzo-0.4.21.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=$do_date
hadoop jar /opt/module/hd/share/hadoop/common/hadoop-lzo-0.4.21.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=$do_date
```







# 1