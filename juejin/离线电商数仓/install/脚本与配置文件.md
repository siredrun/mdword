# 概述

对于shell脚本不太熟的建议看看这个：[shell基础教程](https://juejin.im/post/5e6c29bb518825493038df24)  。

项目中使用到许多脚本和一些配置文件，为了方便管理全部都在此。在服务器中为方便执行，全部都在/opt/module/myscripts下，如使用root账户，就放在/root/bin下。脚本执行的基本三要素：hostname、hosts、ssh免密登录。

脚本文件目录

```
xcall集群整体操作脚本
xsync同步脚本
xsync2同步脚本（除主节点外）
jpsall集群Java实例查看脚本
ct集群时间更新 更新到现在时间
dt集群时间修改
lg日志生成脚本
hd群启hadoop脚本
zk群启zookeeper脚本
kf群启Kafka脚本
f1.conf日志采集Flume Agent
f2.conf日志消费Flume Agent
f1日志采集Flume启动停止脚本
f2日志消费Flume启动停止脚本
keepalived.conf高可用MysqlHA
ods_log加载数据脚本ODS层
dwd_start_log加载数据脚本DWD层启动表
```

关于脚本概述

```
mkdir -p /opt/module/myscripts
vim /etc/profile # 加入下面内容
#SCRIPT_HOME
export SCRIPT_HOME=/opt/module/myscripts
export PATH=$PATH:$SCRIPT_HOME

xsync /etc/profile
source /etc/profile
xsync $SCRIPT_HOME
# 所有脚本（有!#/bin/bash）都要执行下面三步操作，以同步脚本xsync为例
vim $SCRIPT_HOME/xsync # 建立文件，输入内容
chmod +x $SCRIPT_HOME/xsync # 增加权限
xsync $SCRIPT_HOME/xsync # 同步到所有集群
```

# xcall集群整体操作脚本

```sh
#!/bin/bash
#验证参数
if(($#==0))
then
        echo 请传入要执行的命令!
        exit;
fi

echo "要执行的命令是:$@"

#批量执行
for i in hadoop102 hadoop103 hadoop104
do
        echo -----------------------$i---------------------
        echo $i $@
        ssh  $i $@
done
```

# xsync同步脚本

```sh
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for i in hadoop102 hadoop103 hadoop104
do
        echo ------------------- $i --------------
        rsync -av $pdir/$fname $user@$i:$pdir
done
```

# xsync2同步脚本（除主节点外）

```sh
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for i in hadoop103 hadoop104
do
        echo ------------------- $i --------------
        rsync -av $pdir/$fname $user@$i:$pdir
done
```

# jpsall集群Java实例查看脚本

```
#!/bin/bash
for i in hadoop102 hadoop103 hadoop104
do
echo "========== $i ==========" 
ssh $i $JAVA_HOME/bin/jps
done
```

# ct集群时间更新

```sh
#!/bin/bash
#将集群的时间都同步为最新的时间
for i in hadoop102 hadoop103 hadoop104
do
	echo -----------$i---------------
	ssh $i ntpdate -u ntp1.aliyun.com
done
```

# dt集群时间修改

使用 dt '2019-20-35'

```sh
#/bin/bash
#在hadoop102、hadoop103、hadoop104上同步日期为指定的日期
if(($#==0))
then
	echo 请输入要修改的时间!
	exit;
fi

#修改系统时间
for i in hadoop102 hadoop103 hadoop104
do
	echo ------------同步$i时间--------------
	ssh $i "sudo date -s '$@'"
done
```

# lg日志生成脚本

说明：直接执行lg会在hadoop102和hadoop103分别产生2000条共4000条数据，1g 2 500表示每2毫秒产生一条数据，且分别在hadoop102和hadoop103上产生500条数据共1000条数据。

```sh
#!/bin/bash
#在hadoop102,hadoop103产生日志
for i in hadoop102 hadoop103
do
    ssh $i $JAVA_HOME/bin/java -cp /opt/module/log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.sire.appclient.AppMain $1 $2 > /dev/null 2>&1 &	
done
```

# hd群启hadoop脚本

```sh
#!/bin/bash
#hadoop集群的一键启动脚本
if(($#!=1))
then
	echo '请输入start|stop|restart参数!'
	exit;
fi

#只允许传入start和stop参数
if [ $1 = start ] || [ $1 = stop ]
then
	ssh hadoop103 $HADOOP_HOME/sbin/$1-dfs.sh
	ssh hadoop103 $HADOOP_HOME/sbin/$1-yarn.sh
	ssh hadoop102 $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh $1 historyserver
elif [ $1 = restart ]
then
	echo ---------------stop---------------
        ssh hadoop103 $HADOOP_HOME/sbin/stop-dfs.sh
        ssh hadoop103 $HADOOP_HOME/sbin/stop-yarn.sh
        ssh hadoop102 $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh stop historyserver

	echo ---------------start---------------
        ssh hadoop103 $HADOOP_HOME/sbin/start-dfs.sh
        ssh hadoop103 $HADOOP_HOME/sbin/start-yarn.sh
        ssh hadoop102 $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver
else
	echo '请输入start|stop参数!'
fi
```

# zk群启zookeeper脚本

```sh
#!/bin/bash
#hadoop集群的一键启动脚本
if(($#!=1))
then
        echo '请输入start|stop|status|restart!'
        exit;
fi

if [ $1 = start ] || [ $1 = stop ]
then
        echo -----------$1---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh $1
        done
        echo -----------status---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh status
        done
elif [ $1 = status ]
then
        echo -----------status---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh status
        done
elif [ $1 = restart ]
then
        echo -----------stop---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh stop
        done
        echo -----------start---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh start
        done
        echo -----------status---------------
        for i in hadoop102 hadoop103 hadoop104
        do
        		echo -----------$i---------------
                ssh $i $ZK_HOME/bin/zkServer.sh status
        done
else
        echo '请输入start|stop|status|restart!'
fi
```

# kf群启Kafka脚本

```sh
#!/bin/bash
#只接收start和stop参数
if(($#!=1))
then
	echo '请输入start|stop|restart!'
	exit;
fi

if [ $1 = start ]
then
	echo -----------start---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-start.sh -daemon $KF_HOME/config/server.properties
	done
elif [ $1 = stop ]
then
	echo -----------stop---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-stop.sh
	done
elif [ $1 = restart ]
then
	echo -----------stop---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-stop.sh
	done
	echo -----------start---------------
	for i in hadoop102 hadoop103 hadoop104
	do
		echo -----------$i---------------
        ssh $i $KF_HOME/bin/kafka-server-start.sh -daemon $KF_HOME/config/server.properties
	done
else
	echo '请输入start|stop|restart!'
fi
```

# f1.conf日志采集Flume Agent

Flume Agent配置文件*.conf都是放在$FL_HOME/myagents下的vim $FL_HOME/myagents/f1.conf

```
#a1是agent的名称，a1中定义了一个叫r1的source，如果有多个，使用空格间隔
a1.sources = r1
a1.channels = c1 c2

#组名名.属性名=属性值
a1.sources.r1.type=TAILDIR
a1.sources.r1.filegroups=f1
a1.sources.r1.batchSize=1000
#读取/tmp/logs/app-yyyy-mm-dd.log ^代表以xxx开头$代表以什么结尾 .代表匹配任意字符
#+代表匹配任意位置
a1.sources.r1.filegroups.f1=/tmp/logs/^app.+.log$
#JSON文件的保存位置
a1.sources.r1.positionFile=/opt/module/fl/test/log_position.json

#定义拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.root.flume.LogTypeInterceptor$Builder

#定义ChannelSelector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2


#定义chanel
a1.channels.c1.type=org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic=topic_start
a1.channels.c1.parseAsFlumeEvent=false

a1.channels.c2.type=org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic=topic_event
a1.channels.c2.parseAsFlumeEvent=false

#连接组件 同一个source可以对接多个channel，一个sink只能从一个channel拿数据！
a1.sources.r1.channels=c1 c2
```

# f2.conf日志消费Flume Agent

Flume Agent配置文件*.conf都是放在$FL_HOME/myagents下的vim $FL_HOME/myagents/f2.conf

```
配置文件编写
a1.sources = r1 r2
a1.sinks = k1 k2
a1.channels = c1 c2

#配置source
a1.sources.r1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_start
# 从头开始消费
a1.sources.r1.kafka.consumer.auto.offset.reset=earliest
a1.sources.r1.kafka.consumer.group.id=CG_Start

a1.sources.r2.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics=topic_event
# 从头开始消费
a1.sources.r2.kafka.consumer.auto.offset.reset=earliest
a1.sources.r2.kafka.consumer.group.id=CG_Event
#配置channel
a1.channels.c1.type=file
a1.channels.c1.checkpointDir=/opt/module/fl/c1/checkpoint
#启动备用checkpoint
a1.channels.c1.useDualCheckpoints=true
a1.channels.c1.backupCheckpointDir=/opt/module/fl/c1/backupcheckpoint
#event存储的目录
a1.channels.c1.dataDirs=/opt/module/fl/c1/datas


a1.channels.c2.type=file
a1.channels.c2.checkpointDir=/opt/module/fl/c2/checkpoint
a1.channels.c2.useDualCheckpoints=true
a1.channels.c2.backupCheckpointDir=/opt/module/fl/c2/backupcheckpoint
a1.channels.c2.dataDirs=/opt/module/fl/c2/datas


#sink
a1.sinks.k1.type = hdfs
#一旦路径中含有基于时间的转义序列，要求event的header中必须有timestamp=时间戳，如果没有需要将useLocalTimeStamp = true
a1.sinks.k1.hdfs.path = hdfs://hadoop102:9000/origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-

a1.sinks.k1.hdfs.batchSize = 1000

#文件的滚动
#60秒滚动生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 30
#设置每个文件到128M时滚动
a1.sinks.k1.hdfs.rollSize = 134217700
#禁用基于event数量的文件滚动策略
a1.sinks.k1.hdfs.rollCount = 0
#指定文件使用LZO压缩格式
a1.sinks.k1.hdfs.fileType = CompressedStream 
a1.sinks.k1.hdfs.codeC = lzop
#a1.sinks.k1.hdfs.round = true
#a1.sinks.k1.hdfs.roundValue = 10
#a1.sinks.k1.hdfs.roundUnit = second



a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = hdfs://hadoop102:9000/origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-
a1.sinks.k2.hdfs.batchSize = 1000
a1.sinks.k2.hdfs.rollInterval = 30
a1.sinks.k2.hdfs.rollSize = 134217700
a1.sinks.k2.hdfs.rollCount = 0
a1.sinks.k2.hdfs.fileType = CompressedStream 
a1.sinks.k2.hdfs.codeC = lzop
#a1.sinks.k2.hdfs.round = true
#a1.sinks.k2.hdfs.roundValue = 10
#a1.sinks.k2.hdfs.roundUnit = second

#连接组件
a1.sources.r1.channels=c1
a1.sources.r2.channels=c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2
```



# f1日志采集Flume启动停止脚本

```sh
#!/bin/bash
#使用start启动脚本，使用stop停止脚本
if(($#!=1))
then
	echo 请输入start或stop!
	exit;
fi
#定义cmd用来保存要执行的命令
cmd=cmd
if [ $1 = start ]
then
	# 打印的日志放在100以内。
	cmd="nohup $FL_HOME/bin/flume-ng agent -c $FL_HOME/conf/ -n a1 -f $FL_HOME/myagents/f1.conf -Dflume.root.logger=DEBUG,console > /root/f1.log 2>&1 &"

elif [ $1 = stop ]
	then 
		cmd="ps -ef  | grep f1.conf | grep -v grep | awk  '{print \$2}' | xargs kill -9"
else
	echo 请输入start或stop!
fi

#在hadoop102和hadoop103开启采集
for i in hadoop102 hadoop103
do
	ssh $i $cmd
done
```

# f2日志消费Flume启动停止脚本

```bash
#!/bin/bash
#使用start启动脚本，使用stop停止脚本
if(($#!=1))
then
        echo 请输入start或stop或restart!
        exit;
fi
#定义cmd用来保存要执行的命令
cmd=cmd
if [ $1 = start ]
then
        # 打印的日志放在100以内。
        ssh hadoop104 $FL_HOME/bin/flume-ng agent -c $FL_HOME/conf/ -n a1 -f $FL_HOME/myagents/f2.conf -Dflume.root.logger=DEBUG,console > /root/f2.log 2>&1 &

elif [ $1 = stop ]
then
	ssh hadoop104 ps -ef  | grep f2.conf | grep -v grep | awk  '{print \$2}' | xargs kill -9
else
        echo 请输入start或stop或restart!
fi
```

# keepalived.conf高可用MysqlHA

路径：/etc/keepalived/keepalived.conf

```
! Configuration File for keepalived
global_defs {
   router_id MySQL-ha
}
vrrp_instance VI_1 {
    state master # 初始状态
    interface eth0 # 网卡
    virtual_router_id 51 # 虚拟器路由ID
    priority 100 # 优先级
    advert_int 1 # Keepalived心跳间隔
    nopreempt # 只在高优先级配置， 原master恢复之后不重新上位
    authentication {
        auth_type PASS # 认证相关
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.200.100 # 虚拟ip
    }
}
#声明虚拟服务器
virtual_server 192.168.200.100 3306 {
    delay_loop 6
    persistence_timeout 30
    protocol TCP
	# 声明真实服务器
    real_server 172.31.82.15 3306 {
    	notify_down /var/lib/mysql/killkeepalived.sh # 真实服务故障后调用脚本
    	TCP_CHECK {
    		connect_timeout 3 # 超时时间
    		nb_get_retry 1 # 重试次数
    		delay_before_retry 1 # 重试时间间隔
    	}
    }
}
```



# ods_log加载数据脚本ODS层

只能在有安装hive的节点上执行，执行：ods_log 2020-05-20。

```sh
#!/bin/bash
#向ods的两个表中导入每天的数据，为数据创建LZO索引
#接受要导入数据的日期参数,-n可以判断后面的参数是否为赋值，如果赋值，返回true,否则返回false
#为判断的变量名加双引号
if [ -n "$1" ]
then
	 do_date=$1
else
	do_date=$(date -d yesterday +%F)
fi

echo ===日志日期为$do_date===

APP=gmall
sql="
load data inpath '/origin_data/gmall/log/topic_start/$do_date' into table $APP.ods_start_log partition(dt='$do_date');

load data inpath '/origin_data/gmall/log/topic_event/$do_date' into table $APP.ods_event_log partition(dt='$do_date');
"
# 注意hive安装的主机 不能通过ssh hadoop102 hive 类似命令执行，因为会用冲突，所以必须在有安装hive的节点上面执行。
$HIVE_HOME/bin/hive --database gmall -e "$sql" 

# hadoop在所以节点都有安装，所以不用管它
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=$do_date

$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=$do_date
```

# dwd_start_log加载数据脚本DWD层启动表

只能在有安装hive的节点上执行，执行：dwd_start_log 2020-05-20。

```sh
#!/bin/bash

# 定义变量方便修改
APP=gmall
hive=$HIVE_HOME/bin/hive

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
	do_date=$1
else 
	do_date=`date -d "-1 day" +%F`  
fi 

sql="
set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table "$APP".dwd_start_log
PARTITION (dt='$do_date')
select 
   get_json_object(line,'$.mid') mid_id,
   get_json_object(line,'$.uid') user_id,
   get_json_object(line,'$.vc') version_code,
   get_json_object(line,'$.vn') version_name,
   get_json_object(line,'$.l') lang,
   get_json_object(line,'$.sr') source,
   get_json_object(line,'$.os') os,
   get_json_object(line,'$.ar') area,
   get_json_object(line,'$.md') model,
   get_json_object(line,'$.ba') brand,
   get_json_object(line,'$.sv') sdk_version,
   get_json_object(line,'$.g') gmail,
   get_json_object(line,'$.hw') height_width,
   get_json_object(line,'$.t') app_time,
   get_json_object(line,'$.nw') network,
   get_json_object(line,'$.ln') lng,
   get_json_object(line,'$.la') lat,
   get_json_object(line,'$.entry') entry,
   get_json_object(line,'$.open_ad_type') open_ad_type,
   get_json_object(line,'$.action') action,
   get_json_object(line,'$.loading_time') loading_time,
   get_json_object(line,'$.detail') detail,
   get_json_object(line,'$.extend1') extend1
from "$APP".ods_start_log 
where dt='$do_date';"

$hive --database gmall -e "$sql"
```



# 1