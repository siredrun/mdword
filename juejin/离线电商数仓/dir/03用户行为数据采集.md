# 用户行为数据采集

## 日志生成

把log-collector项目打包生成的log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar（带依赖的jar包）上传到linux。

```
java -cp log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.sire.appclient.AppMain 1000 5
...
{"action":"1","ar":"MX","ba":"HTC","detail":"","en":"start","entry":"4","extend1":"","g":"36UE0145@gmail.com","hw":"640*1136","l":"es","la":"22.3","ln":"-49.3","loading_time":"11","md":"HTC-7","mid":"4","nw":"WIFI","open_ad_type":"2","os":"8.0.2","sr":"T","sv":"V2.6.4","t":"1592604392371","uid":"4","vc":"10","vn":"1.2.4"}
mkdir -p /tmp/logs
chmod +x /tmp/logs
xsync /tmp/logs
xcall rm -rf /tmp/logs/*
lg #日志生成脚本
xcall ls /tmp/logs  # 在hadoop102和hadoop103下的/tmp/logs/里面生成app-*.log文件
要执行的命令是:ls /tmp/logs
-----------------------hadoop102---------------------
hadoop102 ls /tmp/logs
app-2020-06-29.log
-----------------------hadoop103---------------------
hadoop103 ls /tmp/logs
app-2020-06-29.log
-----------------------hadoop104---------------------
hadoop104 ls /tmp/logs
ls: cannot access /tmp/logs: No such file or directory

# 通过修改系统时间，模拟生成不同日期的数据
dt '2019-12-15'
------------同步hadoop102时间--------------
Sun Dec 15 00:00:00 CST 2019
------------同步hadoop103时间--------------
Sun Dec 15 00:00:00 CST 2019
------------同步hadoop104时间--------------
Sun Dec 15 00:00:00 CST 2019

lg #日志生成
xcall ls /tmp/logs 
要执行的命令是:ls /tmp/logs
-----------------------hadoop102---------------------
hadoop102 ls /tmp/logs
app-2019-12-15.log
app-2020-06-30.log
-----------------------hadoop103---------------------
hadoop103 ls /tmp/logs
app-2019-12-15.log
app-2020-06-30.log
-----------------------hadoop104---------------------
hadoop104 ls /tmp/logs
ls: cannot access /tmp/logs: No such file or directory

ct # 更新集群时间为现在时刻
-----------hadoop102---------------
30 Jun 09:40:05 ntpdate[15237]: adjust time server 120.25.115.20 offset -0.022089 sec
-----------hadoop103---------------
30 Jun 09:40:11 ntpdate[20614]: adjust time server 120.25.115.20 offset -0.000441 sec
-----------hadoop104---------------
30 Jun 09:40:17 ntpdate[14855]: adjust time server 120.25.115.20 offset -0.022907 sec

```

\>命令解释

当执行一个命令时，如果此条命令有输出，可以使用 > 符号，讲输出定向到某个文件中。此时标注输出就不向屏幕输出了。

如：

```shell
locate javac.1
/usr/java/jdk1.8.0_181-cloudera/man/ja_JP.UTF-8/man1/javac.1
/usr/java/jdk1.8.0_181-cloudera/man/man1/javac.1
locate javac.1 > a.log 
cat a.log 
/usr/java/jdk1.8.0_181-cloudera/man/ja_JP.UTF-8/man1/javac.1
/usr/java/jdk1.8.0_181-cloudera/man/man1/javac.1
```

Linux中的IO设备
在linux中，有三个常用的IO设备
0： 代表标注输入。类似Java中的System.in.scan()，接受用户的输入。
1:   代表标注输出。类似Java中的System.out.print()，接受程序的标注输出（正常输出，默认的输入）。
2:   代表错误输出。类似Java中的System.err.print()，接受程序报错时输出的信息。		
/dev/null : 俗称黑洞，如果输出中消息不希望使用，可以定向输出到此设备。		
格式：命令 > 文件：  执行命令，将命令的标注输出定向到文件中！
命令 > 文件 等价于  命令 1> 文件。如：pwd 1 > a.log 等同于 pwd > a.log 。

pwd 2 >&c.log等价于pwd 1 > c.log 2 >&c.log ： 将pwd的错误消息定向到c.log，没有报错，消息还是使用标注输出在控制台输出！
pwd 1> c.log 2> c.log 等价于 pwd 1> e.log 2>&1  ： pwd程序的标注输出和错误输出都输出到c.log。

```shell
[root@slave2 ~]# pwdw 2 >&a.log
[root@slave2 ~]# cat a.log 
-bash: pwdw: command not found
```

解释：

```
> /dev/null 正常输出到黑洞
2>&1 错误信息变为正常输出
& 后台执行
```

## 日志采集

![](https://user-gold-cdn.xitu.io/2020/7/1/1730965577e0024e?w=670&h=308&f=png&s=16508)

集群规划

| 服务名称        | 子服务 | 服务器  hadoop102 | 服务器  hadoop103 | 服务器  hadoop104 |
| --------------- | ------ | ----------------- | ----------------- | ----------------- |
| Flume(采集日志) | Flume  | √                 | √                 |                   |

开始

```
# 添加两个主题，topic_start分区3副本2，topic_event分区2副本2。
kafka-topics.sh  --zookeeper hadoop102:2181/mykafka --create --partitions 3 --replication-factor 2 --topic topic_start
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 2 --replication-factor 2  --topic topic_event
kafka-topics.sh --zookeeper hadoop102:2181 --list
# 把项目flume-interceptor打包生成的flume-interceptor-1.0-SNAPSHOT.jar放在$FL_HOME/lib下
xcall ls $FL_HOME/lib|grep flume-interceptor-1.0-SNAPSHOT.jar 
flume-interceptor-1.0-SNAPSHOT.jar
flume-interceptor-1.0-SNAPSHOT.jar
flume-interceptor-1.0-SNAPSHOT.jar

xcall ls $FL_HOME/myagents # 保证hadoop102和hadoop103的$FL_HOME/myagents下有Flume Agent配置文件f1.conf
要执行的命令是:ls /opt/module/fl/myagents
-----------------------hadoop102---------------------
hadoop102 ls /opt/module/fl/myagents
f1.conf
-----------------------hadoop103---------------------
hadoop103 ls /opt/module/fl/myagents
f1.conf

# 测试采集
lg 2 500 #每2毫秒产生一条数据，且分别在hadoop102和hadoop103上产生500条数据共1000条数据
xcall ls /tmp/logs # 查看hadoop102和hadoop103有无生成app*.log日志
f1 start # 开始采集hadoop102和hadoop103上的数据
# 使用kafka tool2查看两个主题topic_start和topic_event的“total number of messages”信息总数在收集前后有无增加1000条。关于执行日志请查看/root/f1.log，根据f1采集脚本里的“...EBUG,console > /root/f1.log 2>&1 &”，有错误请根据这个来解决。
```

## 采集总结

Source:数据源在日志文件中，读取日志中的数据，可以使用以下Source。
ExecSource:  可执行一个linux命令，例如tail -f 日志文件，讲读取的到的数据封装为Event。不用，不安全，可能丢数据。
SpoolingDirSource:  可读取一个目录中的文本文件，保证目录中无重名文件，保证目录中文件都是封闭状态，一旦放入目录中，不能再继续写入，每个日志封闭后，才能放入到SpoolingDir，不然agent就故障。
TailDirSource: 接近实时第读取指定的文件，断点续传功能，使用此Source。
Channel：KafkaChannel，基于kafka的副本功能，提供了高可用性！event被存储在kafka中。即便agent挂掉或broker挂掉，依然可以让sink从channel中读取数据。
应用场景：
1、KafkaChannel和sink和source一起使用，单纯作为channel。
2、KafkaChannel+拦截器+Source，只要Source把数据写入到kafka就完成，也是目前使用的场景。
3、KafkaChannel+sink，使用flume将kafka中的数据写入到其他的目的地，例如hdfs。

为例在上述场景工作，KafkaChannel可以配置生产者和消费者的参数！
2、和kafka集群相关的参数
必须的配置：
type=org.apache.flume.channel.kafka.KafkaChannel
kafka.bootstrap.servers=
可选：
kafka.topic： 生成到哪个主题
parseAsFlumeEvent=true(默认)： 
如果parseAsFlumeEvent=true，kafkaChannel会把数据以flume中Event的结构作为参考，
把event中的header+body放入ProducerRecord的value中！

如果parseAsFlumeEvent=false，kafkaChannel会把数据以flume中Event的结构作为参考，
把event中body放入ProducerRecord的value中！

a1.channels.k1.kafka.producer.acks=0		

拦截器
日志数据有两种类型，一种是事件日志，格式 时间戳|{"ap":xx,"cm":{},"et":[{},{}]}；另一种是启动日志，格式：{"en":"start"}。在1个source对接两个KafkaChannel时，需要使用MulitPlexing Channel Selector，将启动日志和事件日志分配到对应Channel。MulitPlexing Channel Selector根据event，header中指定key的映射，来分配。需要自定义拦截器，根据不同的数据类型，在每个Event对象的header中添加key。

功能： 为每个Event，在header中添加key；过滤不符合要求的数据(格式有损坏)
启动日志： {},验证JSON字符串的完整性，是否以{}开头结尾
事件日志：  时间戳|{}
时间戳需要合法：长度13位和都是数字。
验证JSON字符串的完整性，是否以{}开头结尾。

## 日志消费

消费Kafka数据Flume

![](https://user-gold-cdn.xitu.io/2020/7/1/1730966c6f056e9d?w=662&h=312&f=png&s=16761)

集群规划

| 服务名称           | 子服务 | 服务器  hadoop102 | 服务器  hadoop103 | 服务器  hadoop104 |
| ------------------ | ------ | ----------------- | ----------------- | ----------------- |
| Flume（消费Kafka） | Flume  |                   |                   | √                 |

开始

```
xcall ls $FL_HOME/myagents/ | grep 'f2' # 保证集群所有节点都有f2.conf日志消费
f2.conf
f2.conf
f2.conf
#确保kafka tool2查看两个主题topic_start和topic_event的“total number of messages”信息总数大于0，有数据
f2 start #启动日志消费，有错误查看/root/f2.log
kafka-consumer-groups.sh --bootstrap-server hadoop102:9092 --all-groups --list # 查看消费者列表
CG_Event
CG_Start

# 查看是否在hdfs上生成数据
hadoop fs -ls /origin_data/gmall/log/topic_start
Found 1 items
drwxr-xr-x   - root supergroup          0 2020-07-02 06:23 /origin_data/gmall/log/topic_start/2020-07-02
hadoop fs -ls /origin_data/gmall/log/topic_event
Found 1 items
drwxr-xr-x   - root supergroup          0 2020-07-02 06:23 /origin_data/gmall/log/topic_event/2020-07-02
hadoop fs -ls /origin_data/gmall/log/topic_event/2020-07-02
Found 1 items
-rw-r--r--   3 root supergroup      55621 2020-07-02 06:23 /origin_data/gmall/log/topic_event/2020-07-02/logevent-.1593642172673.lzo
hadoop fs -ls /origin_data/gmall/log/topic_start/2020-07-02
Found 1 items
-rw-r--r--   3 root supergroup      15999 2020-07-02 06:23 /origin_data/gmall/log/topic_start/2020-07-02/logstart-.1593642172031.lzo

如果hdfs里面没有生成数据，可能是之前的数据已经消费完毕，偏移量已到最后。两种解决方案：1.把偏移量设置为0最开始的位置；2.重新采集新的日志数据添加到kafka主题。
1.通过kafka-consumer-groups.sh --bootstrap-server hadoop102:9092 --describe --group CG_Start # 查看消费者消费偏移量，如果OFFSET的值不为0，请设置为最初偏移量。OFFSET记录kafka消费的指针，比如消费者有100条数据，但是OFFSET为99，就表示已经消费完了，kafka不会往hdfs里面存储数据。可以通过设置消费者offset的为0来解决。注意：这种方式我暂时不会，待解决！
2.重新采集
1g 2 50 # 生产50*2数据
f1 start # 日志采集脚本启动
f2 start # 日志消费脚本启动
```



## 消费总结

目的：将已经存储在kafka集群中的数据，使用flume上传到HDFS。

数据源在kafka，因此需要使用一个可以对接kafka的source，即kafkaSource。为了安全起见，选择filechannel（安全，效率比memoryChannel低），目的地在hdfs，使用hdfssink。

组件分析
①kafkaSource：kafkaSource就是kafka的一个消费者线程，可以从指定的主题中读取数据。如果希望提供消费的速率，可以配置多个kafkaSource，这些source组成同一个组。kafkaSource在工作时，会检查event的header中有没有timestamp属性，如果没有，
kafkaSource会自动为event添加timestamp=当前kafkaSource所在机器的时间。kafkaSource启动一个消费者，消费者在消费时，默认从分区的最后一个位置消费。

```
必须的配置：
type=org.apache.flume.source.kafka.KafkaSource
kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
kafka.topics=消费的主题
kafka.topics.regex=使用正则表达式匹配主题

可选的配置：
kafka.consumer.group.id=消费者所在的组id
batchSize=一次put多少数据，小于10000
batchDurationMillis=一次put可以最多使用多少时间

和kafkaConsumer相关的属性：kafka.consumer=consumer的属性名
例如：kafka.consumer.auto.offset.reset
```

②fileChannel:  channel中的event是存储在文件中。比memorychannel可靠，但是效率略低。

```
必须的配置：
type=file
checkpointDir=checkpoint线程(负责检查文件中哪些event已经被sink消费了，将这些event的文件删除)保存数据的目录。
useDualCheckpoints=false 是否启动双检查点，如果启动后，会再启动一个备用的checkpoint线程。
如果改为true，还需要设置backupCheckpointDir(备用的checkpoint线程的工作目录)
dataDirs=在哪些目录下保存event，默认为~/.flume/file-channel/data，可以是逗号分割的多个目录。
```

③hdfssink:      hdfssink将event写入到HDFS。目前只支持生成两种类型的文件： text | sequenceFile,这两种文件都可以使用压缩。
写入到HDFS的文件可以自动滚动（关闭当前正在写的文件，创建一个新文件）。基于时间、events的数量、数据大小进行周期性的滚动。支持基于时间和采集数据的机器进行分桶和分区操作。HDFS数据所上传的目录或文件名可以包含一个格式化的转义序列，这个路径或文件名会在上传event时，被自动替换，替换为完整的路径名。使用此Sink要求本机已经安装了hadoop，或持有hadoop的jar包。	必须配置：
type	–	The component type name, needs to be hdfs
hdfs.path	–	HDFS directory path (eg hdfs://namenode/flume/webdata/)

参考：

```
a1.sinks.k1.type = hdfs
#一旦路径中含有基于时间的转义序列，要求event的header中必须有timestamp=时间戳，如果没有需要将useLocalTimeStamp = true
a1.sinks.k1.hdfs.path = hdfs://hadoop101:9000/flume/%Y%m%d/%H/%M
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = logs-

#以下三个和目录的滚动相关，目录一旦设置了时间转义序列，基于时间戳滚动
#是否将时间戳向下舍
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位
a1.sinks.k1.hdfs.roundUnit = minute

#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
a1.sinks.k1.hdfs.batchSize = 100

#以下三个和文件的滚动相关，以下三个参数是或的关系。以下三个参数如果值为0都代表禁用。
#60秒滚动生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 10
#设置每个文件到128M时滚动
a1.sinks.k1.hdfs.rollSize = 134217700
#每写多少个event滚动一次
a1.sinks.k1.hdfs.rollCount = 0
#以不压缩的文本形式保存数据
a1.sinks.k1.hdfs.fileType=DataStream 
```